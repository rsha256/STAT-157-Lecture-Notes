\documentclass[11pt]{article}
\usepackage{header}


\begin{document}
% \setcounter{section}{-1} % 0-index \section
    \title{CS 160 Lecture Notes}
    
    \thispagestyle{empty}

    \begin{center}
    {\LARGE \bf Lecture Notes}\\
    {\large STAT 157 with Alexander Strang}\\
    Spring 2024
    \end{center}
    
    \tableofcontents

    % Note that I use include over input to force new page!
    \include{./lectures/ack.tex}
    \include{./lectures/wk1/lecture1.tex} 
    \include{./lectures/wk1/lecture2.tex}
    \include{./lectures/wk2/lecture3.tex}
    \include{./lectures/wk2/lecture4.tex}
    \include{./lectures/wk3/lecture5.tex}
    \include{./lectures/wk3/lecture6.tex}
    \include{./lectures/wk4/lecture7.tex}
    \include{./lectures/wk4/lecture8.tex}
    \include{./lectures/wk5/lecture9.tex}
    \include{./lectures/wk5/lecture10.tex}
    \include{./lectures/wk6/lecture11.tex}
    \include{./lectures/wk6/lecture12.tex}
    
    \section{Tuesday, February 27th}
    \subsection{Is Information Intrinsic?}
    \begin{minipage}{\textwidth}
    \includepdf[scale=0.7]{./lectures/wk7/isInformationIntrinsic.pdf}
    \end{minipage}
    \includepdf[pages=2-]{./lectures/wk7/isInformationIntrinsic.pdf}

\newpage
    \subsection{Gaussian Examples: Mutual Information for Random Vector}
    \begin{minipage}{\textwidth}
    \includepdf[scale=0.8]{./lectures/wk7/GaussianExamples.pdf}
    \end{minipage}
    \includepdf[pages=2-]{./lectures/wk7/GaussianExamples.pdf}

    \section{Thursday, February 29th}
    \subsection{Information Inequalities II: Data Processing and Fano}
    \begin{minipage}{\textwidth}
    \includepdf[scale=0.7]{./lectures/wk7/InformationInequalitiesII.pdf}
    \end{minipage}
    \includepdf[pages=2-]{./lectures/wk7/InformationInequalitiesII.pdf}

    \subsection{What is Relative Entropy (KL)? Part 1: Review}
    \begin{minipage}{\textwidth}
    \includepdf[scale=0.7]{./lectures/wk7/RelativeEntropy_Part1.pdf}
    \end{minipage}
    \includepdf[pages=2-]{./lectures/wk7/RelativeEntropy_Part1.pdf}

    \subsection{What is Relative Entropy (KL)? Part 2: Statistical Interpretation}
    \begin{minipage}{\textwidth}
    \includepdf[scale=0.7]{./lectures/wk7/RelativeEntropy_Part2.pdf}
    \end{minipage}
    \includepdf[pages=2-]{./lectures/wk7/RelativeEntropy_Part2.pdf}

    
    
    \include{./lectures/wk8/lecture14.tex}
    \include{./lectures/wk8/lecture15.tex}
    \include{./lectures/wk9/lecture16.tex}
    \include{./lectures/wk9/lecture17.tex}
    \include{./lectures/wk10/lecture18.tex}
    \include{./lectures/wk10/lecture19.tex}
    \include{./lectures/wk11/lecture20.tex}
    \include{./lectures/wk11/lecture21.tex}
    \include{./lectures/wk12/lecture22.tex}
    \include{./lectures/wk12/lecture23.tex}
    \include{./lectures/wk13/lecture24.tex}
    \include{./lectures/wk13/lecture25.tex}
    \include{./lectures/wk14/lecture26.tex}
    \include{./lectures/wk14/lecture27.tex}
\end{document}

