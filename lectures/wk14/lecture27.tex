\section{Thursday, April 25th}
\subsection{Logistics}
\begin{itemize}
    \item Slip days with partners is the $\max$
    \item Quiz 8 today + retakes
    \item Project due Monday after reading week
    \item Consulting OH form is now live
\end{itemize}

\subsection{Wrap-Up Activity}
For each of the following, discuss:
\begin{enumerate}
    \item Something you learned that:
    \begin{enumerate}
        \item You found surprising, clarifying, important, useful
        \item You would like to remember/tell a friend
    \end{enumerate}
    \item Something you found confusing (to clarify)
    \item Something you'd like to learn next
\end{enumerate}

\subsubsection{Foundations}
\subsubsection{Entropy}
\begin{itemize}
    \item Entropy as expected surprise/entropy intuition.
\end{itemize}

\subsubsection{Information}
\begin{itemize}
    \item Why KL Divergence is not symmetric, taking one to be the null hypothesis (ground assumption): the order of input matters and why this is a feature and not a bug. Specifically, instead of thinking of it as measuring distance, think of how distinguishable the 2 distributions are. It is not symmetric to say how distinguishable $P$ is from $Q$ is the same as $Q$ from $P$.
    \item In the past, it was just a tool to measure things and used bluntly without thought on why. This class was offered to give intuition to motivate its use in many problems.
    \item The Chernoff-Stein Lemma tells us that the KL Divergence is the rate of decay for how much data do you need to quantify probability of error as small enough.
\end{itemize}

\subsubsection{Processes}
\subsubsection{Communication}
\begin{itemize}
    \item For a discrete memoryless channel Feedback doesn't help (asymptotically).
    \item Appreciation of Architecture.
\end{itemize}

\subsection{Looking Ahead}
\begin{itemize}
    \item Kolmogorov Complexity (more of a CS topic so not focused on here, see CS 70)
    \item Variational Principles for Divergences (more of a STAT 210B topic)
    \item Application to (Kelly) Betting with $\log$ growth rates of evidence (more of a STAT 165 topic).
    \item Utility of perfect information: Information Markets (more of a CS 188 topic).
    \item Estimating Information especially in high-dimensional (intractable) space (more of a DATA C102 topic).
\end{itemize}
